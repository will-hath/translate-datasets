{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# open the file located here, and fix the encoding (many utf-8 characters rendered as \\uXXXX)\n",
    "# /home/ec2-user/translate/responses/reddit-clustering_turkish/response.jsonl\n",
    "with open(\"/home/ec2-user/translate/responses/reddit-clustering_turkish/response.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"/home/ec2-user/translate/responses/reddit-clustering_turkish/response2.jsonl\", \"w\", encoding='utf-8', errors='surrogateescape') as f:\n",
    "    for line in lines:\n",
    "        # Decode Unicode escape sequences, handling surrogates\n",
    "        f.write(line.encode('utf-8', errors='surrogateescape').decode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, I need to upload all of these to huggingface\n",
    "# I need to create a new dataset for each of these\n",
    "import json\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
    "import os\n",
    "def upload_to_huggingface(dataset_name, hf_username, hf_token):\n",
    "    # Authenticate with Hugging Face\n",
    "    api = HfApi()\n",
    "    api.set_access_token(hf_token)\n",
    "\n",
    "    translated_dataset_path = f\"/home/ec2-user/translate/responses/{dataset_name}\"\n",
    "    \n",
    "    # Create a new dataset repository on Hugging Face\n",
    "    new_dataset_name = f\"{hf_username}/{dataset_name}\"\n",
    "    create_repo(new_dataset_name, token=hf_token, repo_type=\"dataset\")\n",
    "    \n",
    "    with open (os.path.join(translated_dataset_path, \"response-fixed.jsonl\"), \"r\") as f:\n",
    "        # make a new jsonl file with\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # make a new json\n",
    "            content = json.loads(line)\n",
    "            \n",
    "\n",
    "\n",
    "    # Upload the translated dataset to the repository\n",
    "    repo_id = f\"{hf_username}/{new_dataset_name}\"\n",
    "    upload_file(\n",
    "        path_or_fileobj=translated_dataset_path,\n",
    "        path_in_repo=\"test.jsonl\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "\n",
    "hf_username = \"willhath\"\n",
    "hf_token = \n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "dataset_name = \"mteb/reddit-clustering\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"/home/ec2-user/translate/responses/spanish-reddit-clustering/response-fixed.jsonl\"\n",
    "output_file_path = \"output.jsonl\"\n",
    "transformed_data = {}\n",
    "\n",
    "# Read the content of the input JSONL file\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        data = json.loads(line)\n",
    "        cluster_id = data[-1][\"cluster_id\"]\n",
    "        \n",
    "        if cluster_id not in transformed_data:\n",
    "            transformed_data[cluster_id] = {\"sentences\": [], \"labels\": []}\n",
    "        \n",
    "        # Extract the sentence from the 'content' field\n",
    "        sentence = data[1][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Extract the label from the 'labels' field\n",
    "        label = data[-1][\"labels\"]\n",
    "        \n",
    "        transformed_data[cluster_id][\"sentences\"].append(sentence)\n",
    "        transformed_data[cluster_id][\"labels\"].append(label)\n",
    "\n",
    "# Write the transformed data to the output JSONL file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for cluster_id, data in transformed_data.items():\n",
    "        output_line = json.dumps(data, ensure_ascii=False)\n",
    "        output_file.write(output_line + '\\n')\n",
    "\n",
    "print(f\"Transformed data saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def split_jsonl(content):\n",
    "    \"\"\"\n",
    "    Splits a jsonl along \"}]\\n\" boundaries\n",
    "    \"\"\"\n",
    "    json_lines = []\n",
    "    json_line = \"\"\n",
    "    lines = content.split(\"}]\\n\")\n",
    "    # replace all \"\\n\" with \"\\\\n\" to avoid splitting on newlines within the JSON\n",
    "    lines = [line.replace(\"\\n\", \"\\\\n\") + \"}]\\n\" for line in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/home/ec2-user/translate/responses/turkish-reddit-clustering/response.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    json_lines = split_jsonl(content)\n",
    "    # now write it to a new json file in the same directory but called response-fixed.jsonl\n",
    "    with open(\"/home/ec2-user/translate/responses/turkish-reddit-clustering/response-fixed.jsonl\", \"w\", encoding='utf-8', errors='surrogateescape') as f:\n",
    "        f.writelines(json_lines)\n",
    "        \n",
    "# run the above code but with all of: french-reddit-clustering            spanish-twentynewsgroups-clustering\n",
    "#french-twentynewsgroups-clustering  swahili-reddit-clustering\n",
    "#german-reddit-clustering            swahili-twentynewsgroups-clustering\n",
    "#german-twentynewsgroups-clustering  turkish-reddit-clustering\n",
    "#      turkish-twentynewsgroups-clustering\n",
    "#spanish-reddit-clustering\n",
    "\n",
    "for dataset_name in [\"french-reddit-clustering\", \"spanish-twentynewsgroups-clustering\", \"french-twentynewsgroups-clustering\", \"swahili-reddit-clustering\", \"german-reddit-clustering\", \"swahili-twentynewsgroups-clustering\", \"german-twentynewsgroups-clustering\", \"turkish-reddit-clustering\", \"turkish-twentynewsgroups-clustering\", \"spanish-reddit-clustering\"]:\n",
    "    with open(f\"/home/ec2-user/translate/responses/{dataset_name}/response.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        json_lines = split_jsonl(content)\n",
    "        # now write it to a new json file in the same directory but called response-fixed.jsonl\n",
    "        with open(f\"/home/ec2-user/translate/responses/{dataset_name}/response-fixed.jsonl\", \"w\", encoding='utf-8', errors='surrogateescape') as f:\n",
    "            f.writelines(json_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lines# Specify the input file and output file\n",
    "input_file_path = '/home/ec2-user/translate/responses/spanish-twentynewsgroups-clustering/response.jsonl'\n",
    "output_file_path = 'output_file.txt'\n",
    "\n",
    "# Read the content of the input file and replace newline characters with spaces\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    file_content = input_file.read().replace('\\n', ' ')\n",
    "\n",
    "# Write the modified content to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(file_content)\n",
    "\n",
    "print(f\"Newline characters replaced and saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASFEDSDF = [\"french-reddit-clustering\",\"spanish-twentynewsgroups-clustering\", \"french-twentynewsgroups-clustering\", \"swahili-reddit-clustering\", \"german-reddit-clustering\", \"swahili-twentynewsgroups-clustering\", \"german-twentynewsgroups-clustering\", \"turkish-reddit-clustering\", \"turkish-twentynewsgroups-clustering\", \"spanish-reddit-clustering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_hf import *\n",
    "\n",
    "# for dataset in the responses directory\n",
    "for dataset_name in [\"spanish-reddit-clustering\"]:\n",
    "    input_file_path = f'/home/ec2-user/translate/responses/{dataset_name}/response.jsonl'\n",
    "    hf_username = 'willhath'\n",
    "    hf_token = \n",
    "    output_file_path = input_file_path.replace(\".jsonl\", \"-processed.jsonl\")\n",
    "    process_jsonl(input_file_path, output_file_path)\n",
    "    upload_to_huggingface(dataset_name, hf_username, hf_token, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /home/ec2-user/translate/BeIR/quora-retrieval/quora-retrieval.py or any data file in the same directory. Couldn't find 'BeIR/quora-retrieval' on the Hugging Face Hub either: FileNotFoundError: Dataset 'BeIR/quora-retrieval' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import mteb/quora-retrieval from hf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 4\u001b[0m quora_retrieval \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mBeIR/quora-retrieval\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/load.py:2128\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2124\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2125\u001b[0m )\n\u001b[1;32m   2127\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2128\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2129\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2130\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2131\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2132\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2133\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2134\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2135\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2136\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2137\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2138\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2139\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2140\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2141\u001b[0m )\n\u001b[1;32m   2143\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/load.py:1814\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1813\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1814\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1815\u001b[0m     path,\n\u001b[1;32m   1816\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1817\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1818\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1819\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1820\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1821\u001b[0m )\n\u001b[1;32m   1822\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/load.py:1507\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1507\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1508\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1509\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /home/ec2-user/translate/BeIR/quora-retrieval/quora-retrieval.py or any data file in the same directory. Couldn't find 'BeIR/quora-retrieval' on the Hugging Face Hub either: FileNotFoundError: Dataset 'BeIR/quora-retrieval' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "# import mteb/quora-retrieval from hf\n",
    "from datasets import load_dataset\n",
    "\n",
    "quora_retrieval = load_dataset(\"BeIR/quora-retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ec2-user/.local/lib/python3.9/site-packages (2.15.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.9/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.25.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/.local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 4686.37it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 728.56it/s]\n",
      "Generating train split:  40%|███▉      | 12999/32747 [01:01<01:33, 210.31 examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1688\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1687\u001b[0m example \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mencode_example(record) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m record\n\u001b[0;32m-> 1688\u001b[0m writer\u001b[39m.\u001b[39;49mwrite(example, key)\n\u001b[1;32m   1689\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:491\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 491\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:449\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    447\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    448\u001b[0m         ]\n\u001b[0;32m--> 449\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[1;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:560\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    559\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[0;32m--> 560\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:578\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 578\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyarrow/ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyarrow/types.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib._datatype_to_pep3118\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/fsspec/implementations/local.py:369\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 369\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mwrite(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1697\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1696\u001b[0m num_shards \u001b[39m=\u001b[39m shard_id \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1697\u001b[0m num_examples, num_bytes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[1;32m   1698\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:587\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 587\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[1;32m    588\u001b[0m \u001b[39m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:449\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    447\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    448\u001b[0m         ]\n\u001b[0;32m--> 449\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[1;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:560\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    559\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[0;32m--> 560\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_writer.py:578\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 578\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyarrow/ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyarrow/types.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib._datatype_to_pep3118\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/fsspec/implementations/local.py:369\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 369\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mwrite(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mnarrativeqa\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2151\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2153\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2154\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2155\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2156\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   2157\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2158\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2159\u001b[0m )\n\u001b[1;32m   2161\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2163\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2164\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 948\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    949\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    950\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    951\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    952\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    953\u001b[0m     )\n\u001b[1;32m    954\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1711\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1711\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1712\u001b[0m         dl_manager,\n\u001b[1;32m   1713\u001b[0m         verification_mode,\n\u001b[1;32m   1714\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1715\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1716\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1717\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1043\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m   1041\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m   1044\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1046\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1048\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m   1050\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1549\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1547\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1548\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1549\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1550\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1551\u001b[0m     ):\n\u001b[1;32m   1552\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   1553\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/builder.py:1706\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1705\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[0;32m-> 1706\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"narrativeqa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
